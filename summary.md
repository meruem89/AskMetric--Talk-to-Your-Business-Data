# ANARIX AI Agent – Project Summary (for New Developers)

This summary gives you everything you need to quickly understand the project and then build a **similar AI-driven BI system** yourself.

---

## 1. What This Project Does

ANARIX AI Agent is a **business intelligence assistant** that lets non-technical users ask **natural-language questions** about their sales and advertising data and get back:

- Automatically generated **PostgreSQL** queries (via **LLaMA 3.1**).
- **Tabular results** from a curated analytics database.
- **Charts/visualizations** (Plotly) and **business insights**.

Example questions it can answer:

- “Which products have the highest ROAS?”
- “Show products where CPC is above $5.”
- “What is the total revenue generated by each product?”

---

## 2. High-Level Architecture

The system is built with these main parts:

- **FastAPI backends**
  - A **pure LLaMA API** for direct NL → SQL execution ([app/main.py]).
  - An **enhanced BI interface** with validation logic and visualizations ([app/query_interface.py]).
- **PostgreSQL database**
  - Stores three main business tables: `ad_sales`, `total_sales`, `eligibility` ([database/models.py]).
- **LLM layer**
  - Talks to a local **Ollama** server running `llama3.1:8b` ([services/llm_service.py]).
  - Converts questions into SQL, with strong constraints and fallbacks.
- **Visualization & response layer**
  - Handles chart generation and standardized JSON responses ([services/visualization_service.py], [services/response_formatter.py]).
- **Data ingestion from Excel**
  - Loads Excel exports into the 3-table schema ([real_data_integration/excel_to_database.py]).

You can think of it as: **User → Question → LLaMA → SQL → PostgreSQL → Data + Charts**.

---

## 3. The Business Data Model

All analysis is centered on three tables defined in [database/models.py]:

1. **ad_sales**
   - Per-product ad performance (revenue, spend, clicks, impressions, units sold).
   - Used for metrics like **CPC**, **ROAS**, **CTR**.

2. **total_sales**
   - Overall product revenue and units over time.
   - Used for **total revenue**, **top-selling products**, etc.

3. **eligibility**
   - Product eligibility/compliance status with reasons.
   - Used for filtering and compliance-related questions.

A `BusinessIntelligenceView` helper defines the canonical way to **join these tables on `item_id`**.

---

## 4. How Real Data Enters the System

Real data comes from **Excel exports** in the project root:

- Product-level ad sales and metrics.
- Product-level total sales and metrics.
- Product-level eligibility.

The converter in [real_data_integration/excel_to_database.py]:

1. Reads the Excel files using pandas.
2. Cleans and normalizes column names.
3. Ensures the `ad_sales`, `total_sales`, and `eligibility` tables exist.
4. Inserts all rows into PostgreSQL using asyncpg.
5. Prints record counts to confirm the load.

This gives you a **clean, normalized, analytics-ready schema** for the LLM to query.

---

## 5. How LLaMA Generates SQL

The LLM logic lives in [services/llm_service.py].

### 5.1. Prompt and Constraints

The service sends a **rich system prompt** to LLaMA via the Ollama API, which includes:

- Full descriptions of the three tables and their columns.
- Exact formulas for:
  - **ROI** – based on `total_sales` and `ad_spend`.
  - **ROAS** – `ad_sales / ad_spend`.
  - **CPC** – `ad_spend / clicks`.
  - **CTR** – `clicks / impressions`.
- Rules for interpreting questions (e.g., what “top 5” means, when to use `SUM` or `AVG`).
- Strict instructions to output **only a SQL query** and to use only those 3 tables.

### 5.2. Validation and Fallbacks

After LLaMA returns SQL, the service:

- Extracts the SQL from the response.
- Checks that it uses only the allowed tables and correct formulas.
- Verifies alignment with the user intent (e.g., presence of `LIMIT` for “top N”).

If anything looks wrong, it falls back to **handcrafted SQL templates** specific to:

- CPC, ROAS, ROI questions.
- Total revenue or ad spend.
- Top-N style rankings.

This design gives you the **flexibility of an LLM** with the **safety of well-defined SQL patterns**.

---

## 6. The FastAPI Apps

### 6.1. Pure LLaMA API – app/main.py

This is a minimal, developer-focused API:

- `POST /query` takes `{ "question": "..." }`.
- Uses `LLMService` to get SQL.
- Executes the exact SQL with asyncpg (tries multiple fetch methods).
- Returns:
  - `result` (data), `record_count`, `query_type`.
  - `generated_sql`.

It also has visualization endpoints that either:

- Query real PostgreSQL tables, or
- Use mock data from [data/mock_data.py] if the DB is unavailable.

### 6.2. Enhanced BI Interface – app/query_interface.py

This is the **user-facing web app**:

- `GET /` renders the query interface HTML ([templates/query_interface.html]).
- `POST /query` handles the full smart pipeline:
  1. Try LLaMA-generated SQL.
  2. If invalid, generate SQL based on business rules (`generate_business_sql`).
  3. Validate and repair SQL (fix tables, columns, syntax).
  4. Run on PostgreSQL with `get_async_connection()`.
  5. If it still fails, fall back to a safe, known-good SQL query.
  6. Build an explanation, visualization, and business insights.

The frontend then uses this JSON to show **tables + charts + explanations**.

---

## 7. Visualization and Insights

Visualizations are handled by [services/visualization_service.py]:

- Result rows → pandas DataFrame.
- Detect numeric columns.
- Build a simple but reliable bar chart (Plotly), typically:
  - X-axis: `item_id` or first column.
  - Y-axis: first numeric metric (e.g., sales, ROAS, CPC).

It also annotates charts with titles and simple business recommendations.

[services/response_formatter.py] provides helper methods to standardize returned JSON.

---

## 8. How a Single Question Flows Through the System

Example: User asks via the web UI, “Which product has the highest CPC?”

1. Browser sends a POST to `/query` in [app/query_interface.py].
2. Backend tries `LLMService.convert_to_sql()`.
3. If the generated SQL is valid, it’s used; otherwise, a CPC-specific template is used.
4. SQL is validated and fixed if needed.
5. PostgreSQL executes the query over `ad_sales`.
6. Results are:
   - Returned as structured JSON.
   - Visualized as a chart (e.g., top CPC by item).
   - Accompanied by textual explanation and insights.

From the user’s perspective: they just asked a question and got **answers + charts + reasoning**.

---

## 9. Running the Project Locally

High-level steps:

1. **Install dependencies**
   - Use `requirements.txt` to install Python packages.

2. **Set up PostgreSQL**
   - Create a `sales_analytics` database and configured user.
   - Use [database/connection.py] / [app/main.py] to create tables.
   - Optionally run [real_data_integration/excel_to_database.py] to load real Excel data.

3. **Run Ollama with LLaMA 3.1:8b**
   - Ensure `llama3.1:8b` is pulled and available on `http://127.0.0.1:11434`.

4. **Run the FastAPI app(s)**
   - Pure API: `uvicorn app.main:app --reload` (port 8000 by default).
   - UI app: `uvicorn app.query_interface:app --reload` (choose a different port, e.g., 8001).

5. **Explore**
   - Visit the UI homepage and start asking questions.
   - Or call the pure `/query` API directly from tools like Postman or curl.

---

## 10. How to Build a Similar Project

To use this as a **template for another BI agent**:

1. **Design your schema**
   - Decide on core tables and metrics (e.g., customer metrics, subscription metrics, logistics data).
   - Implement them in [database/models.py].

2. **Create an ETL pipeline**
   - Build a script like [real_data_integration/excel_to_database.py] to:
     - Read from Excel/CSV or another source.
     - Normalize columns.
     - Load into your schema.

3. **Update the LLM schema prompt**
   - In [services/llm_service.py], describe your tables and metrics.
   - Define formulas and question interpretations for your business domain.

4. **Add domain-specific fallbacks**
   - Implement SQL templates for key KPIs (e.g., churn, LTV, conversion rate).
   - Use the same pattern: validate LLaMA output, then fall back if needed.

5. **Customize visualizations**
   - Extend [services/visualization_service.py] to support your typical charts.
   - For time series, use line charts; for breakdowns, bar or pie charts.

6. **Polish the UI and APIs**
   - Adjust [templates/query_interface.html] and the FastAPI endpoints.
   - Add authentication, logging, error handling for production.

If you follow these steps, you can quickly spin up a **new AI-powered BI system** on top of any structured dataset using this repository as a proven reference.

---

## 11. Final Notes

- For **deep technical details**, see the full documentation in [a.md].
- The project is intentionally structured so that **each part (LLM, DB, API, visualizations, ETL) is replaceable**.
- By understanding the flow described above, you can comfortably maintain this project or create a new one in the same style.
